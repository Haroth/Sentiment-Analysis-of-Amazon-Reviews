{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 4 milions reviews splitted into 3.6mln for training set and 400k for test set. <br>\n",
    "Reviews was splitted into negatives (1-2 stars) and positive (4-5 stars). <br>\n",
    "Entire dataset comes from <a href='https://www.kaggle.com/bittlingmayer/amazonreviews'>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum words in sentence in training set: 257\n",
      "Average words in sentence in training set: 78.48\n",
      "Positive review in training set: 50.00%\n",
      "\n",
      "Maximum words in sentence in test set: 230\n",
      "Average words in sentence in test set: 78.42\n",
      "Positive review in test set: 50.00%\n"
     ]
    }
   ],
   "source": [
    "with open('train.ft.txt', 'r', encoding = 'utf8') as file:\n",
    "    max_len, avg_len, labels_percent, i = 0, 0, 0, 0\n",
    "    for line in file:\n",
    "        line = line.strip().split(' ')\n",
    "        if len(line) - 1 > max_len:\n",
    "            max_len = len(line) - 1 \n",
    "        avg_len += len(line) - 1\n",
    "        labels_percent += int(line[0][9]) - 1\n",
    "        i = i + 1\n",
    "    avg_len /= i\n",
    "    labels_percent /= i\n",
    "    \n",
    "print('Maximum words in sentence in training set: {}'.format(max_len))\n",
    "print('Average words in sentence in training set: {:.2f}'.format(avg_len))\n",
    "print('Positive review in training set: {:.2f}%\\n'.format(labels_percent * 100))\n",
    "\n",
    "with open('test.ft.txt', 'r', encoding = 'utf8') as file:\n",
    "    max_len, avg_len, labels_percent, i = 0, 0, 0, 0\n",
    "    for line in file:\n",
    "        line = line.strip().split(' ')\n",
    "        if len(line) - 1 > max_len:\n",
    "            max_len = len(line) - 1 \n",
    "        avg_len += len(line) - 1\n",
    "        labels_percent += int(line[0][9]) - 1\n",
    "        i = i + 1\n",
    "    avg_len /= i  \n",
    "    labels_percent /= i\n",
    "    \n",
    "print('Maximum words in sentence in test set: {}'.format(max_len))\n",
    "print('Average words in sentence in test set: {:.2f}'.format(avg_len))\n",
    "print('Positive review in test set: {:.2f}%'.format(labels_percent * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the dataset is not skewed. The number of negative and positive examples is equal. The average number of words the in review is around 78, so later i want to pad/truncate the reviews to the lists of 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File for word embeddings is located <a href='https://nlp.stanford.edu/projects/glove/'>here</a>. It comes from Stanford NLP Group. It was trained using Glove unsupervised algorithm on 2 billion tweets which contains 27 billion tokens. It has 1.2 million words embedded for 25, 50, 100 or 200 dimensional vectors. \n",
    "\n",
    "I am using below indexes for special cases:\n",
    "- index 0 as blank word for padding sentences <br>\n",
    "- index 1 as unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model. May take some time\n",
      "Done. 1193515  words loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadGloveModel(gloveFile, vector_size, vocab_size):\n",
    "    print(\"Loading Glove Model. May take some time\")\n",
    "    with open(gloveFile, 'r', encoding = 'utf-8') as file:\n",
    "        i = 2\n",
    "        embeddings = np.zeros((vocab_size, vector_size))\n",
    "        word_to_index = {'': 0, 'unknown': 1}\n",
    "        index_to_word = {0 : '', 1 : 'unknown'}\n",
    "        for line in file:\n",
    "            splitLine = line.split(' ')\n",
    "            word = splitLine[0]\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "            embeddings[i] = np.asarray(splitLine[1:], dtype='float32')\n",
    "            i = i + 1\n",
    "        print(\"Done.\", len(word_to_index),\" words loaded!\")\n",
    "    return embeddings, word_to_index, index_to_word\n",
    "\n",
    "embedding_matrix, word_to_index, index_to_word = loadGloveModel('Glove/glove.twitter.27B.100d.txt', 100, 1193516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunetly the vocabulary of Glove Word Embeddings is too big to pass to the Keras Embedding Layer. Therefore I am creating my own embeddings by training on reviews from 'train.ft.txt' using Word2Vec algoritm. <br><br>\n",
    "I am using a 5 word context with a minimum of 10 words and downsampling the most common words. <br>\n",
    "Words are embedded in 300 dimensional vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from string import punctuation\n",
    "\n",
    "num_features = 300                  \n",
    "min_word_count = 10    \n",
    "context = 5                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    " \n",
    "# creating iterator which is necassery for training on big files\n",
    "class SentencesIterator():\n",
    "    def __init__(self, inputPath):\n",
    "        self.file = open(inputPath, \"r\", encoding='utf-8')\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        line = self.file.readline()\n",
    "        if line is '':\n",
    "            raise StopIteration\n",
    "        stripped = strip_punctuation(line[11:]).lower().strip()\n",
    "        return stripped.split(' ')\n",
    "\n",
    "\n",
    "sentences_iterator = SentencesIterator('train.ft.txt')\n",
    "\n",
    "print(\"Training Word2Vec model\")\n",
    "w2v = Word2Vec(sentences_iterator, workers=4, size=num_features, min_count = min_word_count,\\\n",
    "                 window = context, sample = downsampling, iter=100)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts3\")\n",
    "print(\"Vocabulary size : {}\".format(len(w2v.wv.index2word)))\n",
    "print(\"Top 10 words in vocabulary: {}\".format(w2v.wv.index2word[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating preprocessed data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data files containing raw review text with labels is converted to indexes list. Label is at first place in line, indexes are separated by spaces. <br>\n",
    "0 - negative review, 1 - positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts\")\n",
    "\n",
    "# index 169029 is used as an unknown word\n",
    "voc_size = w2v.wv.vectors.shape[0] #169029\n",
    "\n",
    "# creating dictionary word2index\n",
    "word2index = {}\n",
    "for key in w2v.wv.vocab:\n",
    "    word2index[key] = w2v.wv.vocab[key].index\n",
    "\n",
    "def sentence_to_indexes(sentence, voc_size):\n",
    "    return [word2index.get(word, voc_size) for word in sentence.split(' ')]\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "def create_preprocessed_dataset(train_file, test_file, train_preprocessed_file, test_preprocessed_file):\n",
    "    print('Creating preprocessed train dataset file. May take some time')\n",
    "    with open(train_file, 'r', encoding='utf8') as file_in,\\\n",
    "    open(train_preprocessed_file, 'w') as file_out:\n",
    "        i = 0\n",
    "        for sentence in file_in:\n",
    "            if (sentence == ''):\n",
    "                break\n",
    "            sentence_out = [int(sentence[9]) - 1]\n",
    "            stripped = strip_punctuation(sentence[11:]).lower().strip()\n",
    "            sentence_out += sentence_to_indexes(stripped, voc_size)\n",
    "            print(*sentence_out, sep = ' ', file = file_out)\n",
    "            i = i + 1\n",
    "            if (i % 900000 == 0):\n",
    "                print('{}% loaded'.format(100 * i / 3600000))\n",
    "    print('Done!')\n",
    "                \n",
    "    print('Creating preprocessed test dataset file. May take some time')\n",
    "    i = 0\n",
    "    with open(test_file, 'r', encoding='utf8') as file_in,\\\n",
    "    open(test_preprocessed_file, 'w') as file_out:\n",
    "        for sentence in file_in:\n",
    "            if (sentence == ''):\n",
    "                break\n",
    "            sentence_out = [int(sentence[9]) - 1]\n",
    "            stripped = strip_punctuation(sentence[11:]).lower().strip()\n",
    "            sentence_out += sentence_to_indexes(stripped, voc_size)\n",
    "            print(*sentence_out, sep = ' ', file = file_out)\n",
    "            i = i + 1\n",
    "            if (i % 100000 == 0):\n",
    "                print('{}% loaded'.format(100 * i / 400000))\n",
    "            \n",
    "    print('Done!')\n",
    "\n",
    "create_preprocessed_dataset('train.ft.txt', 'test.ft.txt', 'train_preprocessed2.txt', 'test_preprocessed2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are too big to keep it in RAM and feed to training models. Therefore, it is necessary to create generators that will feed Keras models directly from preprocessed files. <br>\n",
    "\n",
    "Train examples - max_length = 257, average_length = 78.46 <br>\n",
    "Test examples - max_length = 230, average_length = 78.41 <br>\n",
    "\n",
    "- The average generator works by averaging all vectors in the sentence. <br>\n",
    "- In the sentence generator I am padding all shorter senteces to 100 length and truncating the longer ones. The list of 100 words contains integer indexes of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts\")\n",
    "vec_size = 300\n",
    "embedding_matrix = np.append(w2v.wv.vectors, np.zeros((1, vec_size)), axis=0)\n",
    "\n",
    "def indexes_to_vectors(indexes):\n",
    "    return [embedding_matrix[int(index)] for index in indexes]\n",
    "\n",
    "def average_generator(inputPath, vector_size, batch_size):\n",
    "    with open(inputPath, \"r\") as file:\n",
    "        while True:\n",
    "            i = 0\n",
    "            X = np.zeros((batch_size, vector_size))\n",
    "            Y = np.zeros((batch_size,))\n",
    "            while i < batch_size:\n",
    "                line = file.readline()\n",
    "                if line == '':\n",
    "                    file.seek(0)\n",
    "                    line = file.readline()\n",
    "                line = line.strip().split(\" \")    \n",
    "                Y[i] = line[0]\n",
    "                X[i] = np.average(indexes_to_vectors(line[1:]), axis=0)\n",
    "                i = i + 1\n",
    "            yield (X, Y)\n",
    "\n",
    "Tx = 100\n",
    "            \n",
    "def sentences_generator(inputPath, batch_size):\n",
    "    with open(inputPath, \"r\") as file:\n",
    "        while True:\n",
    "            i = 0\n",
    "            X = np.zeros((batch_size, Tx))\n",
    "            Y = np.zeros((batch_size,))\n",
    "            while i < batch_size:\n",
    "                line = file.readline()\n",
    "                if line == '':\n",
    "                    file.seek(0)\n",
    "                    line = file.readline()\n",
    "                line = line.strip().split(\" \")    \n",
    "                Y[i] = line[0]\n",
    "                sentence_length = len(line[1:])\n",
    "                if (sentence_length <= 100):\n",
    "                    X[i, -sentence_length:] = line[1:]\n",
    "                else:\n",
    "                    X[i, :] = line[1:101]\n",
    "                i = i + 1\n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Vector Model (Benchmark Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple model whose operation is based on averaging all words embeddings in a sentence. The averaged vector is further fed to the MLP network. Effectiveness of the algorithm will not be good, but it will be useful on evaluating the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 300\n",
    "batch_size = 2500\n",
    "train_examples = 3600000\n",
    "test_examples = 400000\n",
    "\n",
    "def average_model(vec_size):\n",
    "    X_inp = Input(shape = (vec_size,))\n",
    "    X = Dense(128, activation = 'relu')(X_inp)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(32, activation = 'relu')(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(1, activation = 'sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs = X_inp, outputs = X)\n",
    "    return model\n",
    "\n",
    "\n",
    "train_gen = average_generator('train_preprocessed2.txt', vec_size, batch_size)\n",
    "test_gen = average_generator('test_preprocessed2.txt', vec_size, batch_size) \n",
    "\n",
    "model = average_model(vec_size)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.fit_generator(train_gen, steps_per_epoch = train_examples // batch_size,\n",
    "                    validation_data=test_gen, validation_steps = test_examples // batch_size, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epoch 1:  loss: 0.4943 - acc: 0.7587 - val_loss: 0.4584 - val_acc: 0.7821\n",
    "- Epoch 2:  loss: 0.4545 - acc: 0.7860 - val_loss: 0.4326 - val_acc: 0.7981\n",
    "- Epoch 15: loss: 0.4020 - acc: 0.8164 - val_loss: 0.3835 - val_acc: 0.8253\n",
    "\n",
    "The benchmark model achived 82.5% accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model (custom word embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First recurrential model with 1 layer of LSTM (128 hidden states) which is then fed into 2 Dense layers. The model contains an Embedding Layer which transform the lists of 100 words into the corresponding vectors. <br>\n",
    "The model is using my own word embeddings of 169029 words into 300d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts\")\n",
    "embedding_matrix = w2v.wv.vectors\n",
    "embedding_matrix = np.append(w2v.wv.vectors, np.zeros((1, vec_size)), axis=0)\n",
    "print(\"Shape of embedding matrix: \", embedding_matrix.shape)\n",
    "voc_size = embedding_matrix.shape[0]\n",
    "vec_size = embedding_matrix.shape[1]\n",
    "\n",
    "embedding_layer = Embedding(voc_size, vec_size, trainable=False, weights = [embedding_matrix])\n",
    "\n",
    "def lstm_model():\n",
    "    X_inp = Input(shape = (100,))\n",
    "    X = embedding_layer(X_inp)\n",
    "    X = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2)(X)\n",
    "    X = Dense(10, activation = 'relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(1, activation = 'sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs = X_inp, outputs = X)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "batch_size = 2000\n",
    "train_examples = 3600000\n",
    "test_examples = 400000\n",
    "\n",
    "train_gen = sentences_generator('train_preprocessed2.txt', batch_size)\n",
    "test_gen = sentences_generator('test_preprocessed2.txt', batch_size) \n",
    "\n",
    "model = lstm_model()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.fit_generator(train_gen, steps_per_epoch = train_examples // batch_size,\n",
    "                    validation_data=test_gen, validation_steps = test_examples // batch_size, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epoch 1: 1827s - loss: 0.4579 - acc: 0.7835 - val_loss: 0.3086 - val_acc: 0.8662\n",
    "- Epoch 2: 1816s - loss: 0.3072 - acc: 0.8704 - val_loss: 0.2443 - val_acc: 0.8973\n",
    "\n",
    "The model achived 89.7% accuracy on the test set. It is good result, considering that it is only 2 epochs and training the word embeddings did not take a long time.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the model is identical with the one above, but this one is using 100d Glove word embeddings. The embedding matrix is too big to feed into the embedding layer. Therefore transforming words indexes is done in new generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 100\n",
    "vec_size = embedding_matrix.shape[1]\n",
    "\n",
    "def indexes_to_vectors(indexes):\n",
    "    return [embedding_matrix[int(index)] for index in indexes]\n",
    "            \n",
    "def vectors_generator(inputPath, batch_size):\n",
    "    with open(inputPath, \"r\") as file:\n",
    "        while True:\n",
    "            i = 0\n",
    "            X = np.zeros((batch_size, Tx, vec_size))\n",
    "            Y = np.zeros((batch_size,))\n",
    "            while i < batch_size:\n",
    "                line = file.readline()\n",
    "                if line == '':\n",
    "                    file.seek(0)\n",
    "                    line = file.readline()\n",
    "                line = line.strip().split(\" \")    \n",
    "                Y[i] = line[0]\n",
    "                sentence_length = len(line[1:])\n",
    "                if (sentence_length <= 100):\n",
    "                    X[i, -sentence_length:] = indexes_to_vectors(line[1:])\n",
    "                else:\n",
    "                    X[i] = indexes_to_vectors(line[1:101])\n",
    "                i = i + 1\n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model():\n",
    "    X_inp = Input(shape = (100, 100))\n",
    "    X = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2)(X_inp)\n",
    "    X = Dense(10, activation = 'relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(1, activation = 'sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs = X_inp, outputs = X)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "batch_size = 2000\n",
    "train_examples = 3600000\n",
    "test_examples = 400000\n",
    "\n",
    "train_gen = vectors_generator('train_preprocessed.txt', batch_size)\n",
    "test_gen = vectors_generator('test_preprocessed.txt', batch_size) \n",
    "\n",
    "model = lstm_model()\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.fit_generator(train_gen, steps_per_epoch = train_examples // batch_size,\n",
    "                    validation_data=test_gen, validation_steps = test_examples // batch_size, epochs = 10)\n",
    "\n",
    "#model.save('my_model.h5')\n",
    "#model = keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epoch 1: loss: 0.3033 - acc: 0.8720 - val_loss: 0.2074 - val_acc: 0.9171\n",
    "- Epoch 2: loss: 0.2184 - acc: 0.9152 - val_loss: 0.1802 - val_acc: 0.9295\n",
    "- Epoch 10: loss: 0.1680 - acc: 0.9377 - val_loss: 0.1470 - val_acc: 0.9445\n",
    "\n",
    "The model achived 94.5% accuracy. It is really good score comparing to others models on Kaggle. Especially considering that it is only 3 layer model. <br>\n",
    "\n",
    "I want to get deeper analysis of the model performance. Therefrom I am creating another generator to yield only test data without labels to the predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "model = keras.models.load_model('my_model.h5')\n",
    "\n",
    "Tx = 100\n",
    "vec_size = embedding_matrix.shape[1]\n",
    "batch_size = 2000\n",
    "test_examples = 400000\n",
    "\n",
    "def indexes_to_vectors(indexes):\n",
    "    return [embedding_matrix[int(index)] for index in indexes]\n",
    "            \n",
    "def vectors_generator(inputPath, batch_size):\n",
    "    with open(inputPath, \"r\") as file:\n",
    "        while True:\n",
    "            i = 0\n",
    "            X = np.zeros((batch_size, Tx, vec_size))\n",
    "            while i < batch_size:\n",
    "                line = file.readline()\n",
    "                if line == '':\n",
    "                    file.seek(0)\n",
    "                    line = file.readline()\n",
    "                line = line.strip().split(\" \")    \n",
    "                sentence_length = len(line[1:])\n",
    "                if (sentence_length <= 100):\n",
    "                    X[i, -sentence_length:] = indexes_to_vectors(line[1:])\n",
    "                else:\n",
    "                    X[i] = indexes_to_vectors(line[1:101])\n",
    "                i = i + 1\n",
    "            yield X\n",
    "            \n",
    "test_gen = vectors_generator('test_preprocessed.txt', batch_size) \n",
    "\n",
    "predictions = model.predict_generator(test_gen, test_examples // batch_size)\n",
    "\n",
    "with open('test_preprocessed2.txt', 'r') as file:\n",
    "    Y_true = []\n",
    "    for line in file:\n",
    "        Y_true.append(line[0])       \n",
    "Y_true = np.asarray(Y_true, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.94512\n",
      "Area under the ROV curve score:  0.9451487751290625\n",
      "Confusion matrix:\n",
      "[[189828  11780]\n",
      " [ 10172 188220]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.94      0.95    201608\n",
      "         1.0       0.94      0.95      0.94    198392\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    400000\n",
      "   macro avg       0.95      0.95      0.95    400000\n",
      "weighted avg       0.95      0.95      0.95    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "print('Accuracy score: ', accuracy_score(np.round(predictions), Y_true))\n",
    "print('Area under the ROV curve score: ', roc_auc_score(np.round(predictions), Y_true))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(np.round(predictions), Y_true))\n",
    "print('Classification report:')\n",
    "print(classification_report(np.round(predictions), Y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seem to get errors equal on positive/negative examples. It is slightly less precise on the positive reviews. If I would like to further improve the model performance I would try to implement a different architecture or try to perform an error analysis to see what types of reviews are the most difficult to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
